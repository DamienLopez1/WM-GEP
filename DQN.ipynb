{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Various auxiliary utilities \"\"\"\n",
    "import math\n",
    "from os.path import join, exists\n",
    "import torch\n",
    "from torch import optim\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from models import MDRNN,MDRNNCell, VAE, Controller,Dtild,HiddenVAE\n",
    "from models.mdrnn import gmm_loss\n",
    "\n",
    "import gym\n",
    "import gym.envs.box2d\n",
    "import random\n",
    "import gym_minigrid\n",
    "from gym_minigrid.wrappers import *\n",
    "from gym_minigrid.window import Window\n",
    "import cv2\n",
    "from itertools import chain\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import neighbors\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from collections import namedtuple\n",
    "from utils import dqn_utils\n",
    "\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Q learning stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASIZE, LSIZE, RSIZE, RED_SIZE, SIZE = 1, 32, 256, 64, 64\n",
    "\n",
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    ('state', 'action', 'next_state', 'reward')\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "learning_rate = 0.0001\n",
    "discount_rate = 0.99\n",
    "gamma = 0.99\n",
    "exploration_decay_rate = 0.001\n",
    "memory_size = 100000\n",
    "target_update = 10\n",
    "\n",
    "\n",
    "exploration_rate = 0.8\n",
    "max_exploration_rate = 0.8\n",
    "min_exploration_rate = 0.1\n",
    "device = 'cuda'\n",
    "\n",
    "Controllerstrategy = dqn_utils.EpsilonGreedyStrategy(max_exploration_rate, min_exploration_rate, exploration_decay_rate)\n",
    "Controlleragent = dqn_utils.Agent(Controllerstrategy,'controller', device)\n",
    "Controllermemory = dqn_utils.ReplayMemory(memory_size)\n",
    "\n",
    "#2. Initialise policy network with random weights\n",
    "\n",
    "Controllerpolicy_net = dqn_utils.DQN(288, 32,6,dropout= True, dropout_prob= 0.2).to(device)\n",
    "\n",
    "Controllertarget_net = dqn_utils.DQN(288, 32,6,dropout= True, dropout_prob= 0.2).to(device)\n",
    "Controllertarget_net.load_state_dict(Controllerpolicy_net.state_dict())\n",
    "Controllertarget_net.eval()\n",
    "Controlleroptimizer = optim.Adam(params=Controllerpolicy_net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other functions used by other things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_continuous_policy(action_space, seq_len, dt):\n",
    "    \"\"\" Sample a continuous policy.\n",
    "\n",
    "    Atm, action_space is supposed to be a box environment. The policy is\n",
    "    sampled as a brownian motion a_{t+1} = a_t + sqrt(dt) N(0, 1).\n",
    "\n",
    "    :args action_space: gym action space\n",
    "    :args seq_len: number of actions returned\n",
    "    :args dt: temporal discretization\n",
    "\n",
    "    :returns: sequence of seq_len actions\n",
    "    \"\"\"\n",
    "    actions = [action_space.sample()]\n",
    "    for _ in range(seq_len):\n",
    "        daction_dt = np.random.randn(*actions[-1].shape)\n",
    "        actions.append(\n",
    "            np.clip(actions[-1] + math.sqrt(dt) * daction_dt,\n",
    "                    action_space.low, action_space.high))\n",
    "    return actions\n",
    "\n",
    "def save_checkpoint(state, is_best, filename, best_filename):\n",
    "    \"\"\" Save state in filename. Also save in best_filename if is_best. \"\"\"\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        torch.save(state, best_filename)\n",
    "\n",
    "def flatten_parameters(params):\n",
    "    \"\"\" Flattening parameters.\n",
    "\n",
    "    :args params: generator of parameters (as returned by module.parameters())\n",
    "\n",
    "    :returns: flattened parameters (i.e. one tensor of dimension 1 with all\n",
    "        parameters concatenated)\n",
    "    \"\"\"\n",
    "    return torch.cat([p.detach().view(-1) for p in params], dim=0).cpu().numpy()\n",
    "\n",
    "def unflatten_parameters(params, example, device):\n",
    "    \"\"\" Unflatten parameters.\n",
    "\n",
    "    :args params: parameters as a single 1D np array\n",
    "    :args example: generator of parameters (as returned by module.parameters()),\n",
    "        used to reshape params\n",
    "    :args device: where to store unflattened parameters\n",
    "\n",
    "    :returns: unflattened parameters\n",
    "    \"\"\"\n",
    "    params = torch.Tensor(params).to(device)\n",
    "    idx = 0\n",
    "    unflattened = []\n",
    "    for e_p in example:\n",
    "        unflattened += [params[idx:idx + e_p.numel()].view(e_p.size())]\n",
    "        idx += e_p.numel()\n",
    "    return unflattened\n",
    "\n",
    "def load_parameters(params, controller):\n",
    "    \"\"\" Load flattened parameters into controller.\n",
    "\n",
    "    :args params: parameters as a single 1D np array\n",
    "    :args controller: module in which params is loaded\n",
    "    \"\"\"\n",
    "    proto = next(controller.parameters())\n",
    "    params = unflatten_parameters(\n",
    "        params, controller.parameters(), proto.device)\n",
    "\n",
    "    for p, p_0 in zip(controller.parameters(), params):\n",
    "        p.data.copy_(p_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rollout Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutGenerator(object):\n",
    "    \"\"\" Utility to generate rollouts.\n",
    "\n",
    "    Encapsulate everything that is needed to generate rollouts in the TRUE ENV\n",
    "    using a controller with previously trained VAE and MDRNN.\n",
    "\n",
    "    :attr vae: VAE model loaded from mdir/vae\n",
    "    :attr mdrnn: MDRNN model loaded from mdir/mdrnn\n",
    "    :attr controller: Controller, either loaded from mdir/ctrl or randomly\n",
    "        initialized\n",
    "    :attr env: instance of the CarRacing-v0 gym environment\n",
    "    :attr device: device used to run VAE, MDRNN and Controller\n",
    "    :attr time_limit: rollouts have a maximum of time_limit timesteps\n",
    "    \"\"\"\n",
    "    def __init__(self, mdir, device, time_limit,number_goals,hiddengoals:bool, curiosityreward = bool):\n",
    "        \"\"\" Build vae, rnn, controller and environment. \"\"\"\n",
    "        # Loading world model and vae\n",
    "        vae_file, rnn_file, ctrl_file,Dtild_file,hiddenvae_file = \\\n",
    "            [join(mdir, m, 'best.tar') for m in ['vae', 'mdrnn', 'ctrl','dtild','hiddenvae']]\n",
    "\n",
    "        assert exists(vae_file) and exists(rnn_file),\\\n",
    "            \"Either vae or mdrnn is untrained.\"\n",
    "\n",
    "        vae_state, rnn_state,hiddenvae_state = [\n",
    "            torch.load(fname, map_location={'cuda:0': str(device)})\n",
    "            for fname in (vae_file, rnn_file,hiddenvae_file)]\n",
    "\n",
    "        for m, s in (('VAE', vae_state), ('MDRNN', rnn_state),('HiddenVAE',hiddenvae_state)):\n",
    "            print(\"Loading {} at epoch {} \"\n",
    "                  \"with test loss {}\".format(\n",
    "                      m, s['epoch'], s['precision']))\n",
    "\n",
    "        self.vae = VAE(3, LSIZE).to(device)\n",
    "        self.vae.load_state_dict(vae_state['state_dict'])\n",
    "        \n",
    "        self.HiddenVAE = HiddenVAE(256, LSIZE).to(device)\n",
    "        self.HiddenVAE.load_state_dict(hiddenvae_state['state_dict'])\n",
    "\n",
    "        self.mdrnn = MDRNNCell(LSIZE, ASIZE, RSIZE, 5).to(device)\n",
    "        self.mdrnn.load_state_dict(\n",
    "            {k.strip('_l0'): v for k, v in rnn_state['state_dict'].items()})\n",
    "        \n",
    "        self.mdrnnBIG = MDRNN(LSIZE, ASIZE, RSIZE, 5).to(device)\n",
    "        self.mdrnnBIG.load_state_dict(rnn_state[\"state_dict\"])\n",
    "        \n",
    "        \n",
    "        self.controller = Controller(256, 256, 6).to(device)\n",
    "        self.fmodel = Dtild(32,1, 32).to(device)\n",
    "                \n",
    "        # load controller if it was previously saved\n",
    "        if exists(ctrl_file):\n",
    "            ctrl_state = torch.load(ctrl_file, map_location={'cuda:0': str(device)})\n",
    "            print(\"Loading Controller with reward {}\".format(\n",
    "                ctrl_state['reward']))\n",
    "            self.controller.load_state_dict(ctrl_state['state_dict'])\n",
    "\n",
    "        self.env = gym.make('MiniGrid-MultiRoom-N6-v0')\n",
    "        \n",
    "        self.device = device\n",
    "        self.number_goals = number_goals\n",
    "        self.time_limit = time_limit\n",
    "        \n",
    "        #self.model = neighbors.KNeighborsRegressor(n_neighbors = 5)        \n",
    "        # We initialize the inner non parametric model\n",
    "        \n",
    "        self.rnn_state = rnn_state\n",
    "        self.curiosityreward = curiosityreward\n",
    "        self.hiddengoals = hiddengoals\n",
    "        \n",
    "        if hiddengoals:\n",
    "            self.Controllerpolicy_net = dqn_utils.DQN(288, 256,6,dropout= True, dropout_prob= 0.2).to(device)\n",
    "            self.Controllertarget_net = dqn_utils.DQN(288, 256,6,dropout= True, dropout_prob= 0.2).to(device)\n",
    "        else:\n",
    "            self.Controllerpolicy_net = dqn_utils.DQN(288, 32,6,dropout= True, dropout_prob= 0.2).to(device)\n",
    "            self.Controllertarget_net = dqn_utils.DQN(288, 32,6,dropout= True, dropout_prob= 0.2).to(device)\n",
    "        \n",
    "        self.Controllertarget_net.load_state_dict(Controllerpolicy_net.state_dict())\n",
    "        self.Controllertarget_net.eval()\n",
    "        self.Controlleroptimizer = optim.Adam(params=Controllerpolicy_net.parameters(), lr=learning_rate)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def rollout(self, params, render=False):\n",
    "        \"\"\" Execute a rollout and returns minus cumulative reward.\n",
    "\n",
    "        Load :params: into the controller and execute a single rollout. This\n",
    "        is the main API of this class.\n",
    "\n",
    "        :args params: parameters as a single 1D np array\n",
    "\n",
    "        :returns: minus cumulative reward\n",
    "        \"\"\"\n",
    "        # copy params into the controller\n",
    "        if params is not None:\n",
    "            load_parameters(params, self.fmodel)\n",
    "        optimizer = optim.Adam(params= self.fmodel.parameters(), lr=0.0001)\n",
    "        MDRNNoptimizer = torch.optim.RMSprop(self.mdrnnBIG.parameters(), lr=1e-3, alpha=.9)\n",
    "        MDRNNoptimizer.load_state_dict(self.rnn_state[\"optimizer\"])\n",
    "        \n",
    "        \n",
    "        \n",
    "        zstate_list = []\n",
    "        obs = self.env.reset()\n",
    "        \n",
    "        \n",
    "        obs = obs['image']\n",
    "        expl_rate = 0.5\n",
    "        print(obs.shape)\n",
    "        # This first render is required !\n",
    "        self.env.render()\n",
    "\n",
    "                \n",
    "        hidden = [\n",
    "            torch.zeros(1, RSIZE).to(self.device)\n",
    "            for _ in range(2)]\n",
    "        \n",
    "        z = self.tolatent(obs)\n",
    "        \n",
    "        i = 0\n",
    "        \n",
    "        #Bootstrapping\n",
    "        while True:\n",
    "           \n",
    "            action = random.randrange(6)\n",
    "            \n",
    "            _,hidden,z,zh  = self.transform_obs_hidden(obs, hidden, action)\n",
    "            \n",
    "            obs, reward, done, _ = self.env.step(action)\n",
    "            obs = obs['image']\n",
    "            \n",
    "            if self.hiddengoals:\n",
    "                zstate_list.append(np.array(hidden[0].cpu().detach().numpy()))#if we use pure hidden\n",
    "            else:\n",
    "                zstate_list.append(np.array(z.cpu().detach().numpy()))#if we use latent_space\n",
    "            \n",
    "            i+=1\n",
    "            #if render:\n",
    "            self.env.render()\n",
    "            \n",
    "            print(i)\n",
    "            \n",
    "            \n",
    "            if i > self.time_limit:\n",
    "                break\n",
    "                #return action,hidden,z_list\n",
    "    \n",
    "       \n",
    "        \n",
    "        #visitationarray = np.zeros((25,25))\n",
    "        \n",
    "        \n",
    "        s = obs\n",
    "        goal_achieved_list = []\n",
    "        final_hidden_CS_list = []\n",
    "        loss_list = []\n",
    "        WM_loss= []\n",
    "        rollout_reward = []\n",
    "        \n",
    "        #Goal Exploration\n",
    "        for c in range(self.number_goals):\n",
    "            zstate_list = np.array(zstate_list) \n",
    "            zstate_list = zstate_list.squeeze(1)\n",
    "            kde = scipy.stats.gaussian_kde(zstate_list.T)\n",
    "            \n",
    "            z_goal = sampling_method(kde)\n",
    "            z_goal = torch.tensor([z_goal],dtype = torch.float32).to(self.device) #controller requires both as tensors\n",
    "            \n",
    "            \n",
    "            if not self.hiddengoals:\n",
    "                z_goal_obs = self.vae.decoder(z_goal)\n",
    "                z_goal_obs = z_goal_obs.reshape(7,7,3)\n",
    "                z_goal_obs = np.array(z_goal_obs.cpu().detach())\n",
    "\n",
    "                plt9 = plt.figure('Zgoal')\n",
    "                plt.cla()\n",
    "                sn.heatmap(z_goal_obs[:,:,0],cmap = 'Reds', annot=True,cbar = False).invert_yaxis()\n",
    "            \n",
    "            \n",
    "            total_reward = 0\n",
    "            total_loss = 0\n",
    "            goal_loss = []\n",
    "            \n",
    "            \n",
    "            \n",
    "            scur_rollout = []\n",
    "            snext_rollout = []\n",
    "            r_rollout = []\n",
    "            d_rollout = []\n",
    "            act_rollout = []\n",
    "            \n",
    "            zstate_list = zstate_list[:,np.newaxis,:]\n",
    "            zstate_list = zstate_list.tolist()\n",
    "            \n",
    "            \n",
    "            for goalattempts in range(100):\n",
    "                latent_mu,logsigma,z = self.tolatent(s)\n",
    "                #visitationarray[self.env.agent_pos[0],self.env.agent_pos[1]] += 1\n",
    "                state = torch.cat((torch.cat((hidden[0].detach(),z), dim=1),z_goal), dim=1)\n",
    "                h = []\n",
    "                print('C, goalattempt number', c,goalattempts)\n",
    "                m = Controlleragent.select_action(torch.cat((hidden[0].detach(),z), dim=1), z_goal, Controllerpolicy_net, expl_rate)\n",
    "                \n",
    "                hmus,hsigmas,hlogpi, zt1 = self.predict_next(s, hidden,m) #gets mean, standard deviation and  pi, next latent of prediction of next latent obs\n",
    "                _,hidden,z,zh = self.transform_obs_hidden(s,hidden,m) #gets next hidden , current latent obs, prediction of next latent obs\n",
    "                \n",
    "                \n",
    "                if self.hiddengoals:\n",
    "                    zstate_list.append(np.array(hidden[0].cpu().detach().numpy()))#if we use pure hidden\n",
    "                else:\n",
    "                    zstate_list.append(np.array(z.cpu().detach().numpy()))#if we use latent_space\n",
    "                \n",
    "                predicted_next_obs = self.vae.decoder(zh)\n",
    "                predicted_next_obs = predicted_next_obs.reshape(7,7,3)\n",
    "                p = np.array(predicted_next_obs.cpu().detach())\n",
    "                \n",
    "                \n",
    "                #print('predicted obs', predicted_next_obs)\n",
    "                plt5 = plt.figure('Predicted obs')\n",
    "                plt.cla()\n",
    "                sn.heatmap(p[:,:,0],cmap = 'Reds', annot=True,cbar = False).invert_yaxis()\n",
    "                \n",
    "                \n",
    "                s,_,_,_ = self.env.step(m) #use the action sampled and see if we get to the goal we wanted or how close we got to the goal\n",
    "                \n",
    "                \n",
    "                s = s['image']\n",
    "                \n",
    "                next_mu,next_logsigma,next_z = self.tolatent(s)\n",
    "                next_state = torch.cat((torch.cat((hidden[0].detach(),next_z), dim=1),z_goal), dim=1)\n",
    "                \n",
    "                #print('Actual result', torch.tensor(s,dtype = torch.float32))\n",
    "                plt6 = plt.figure('Actual obs')\n",
    "                plt.cla()\n",
    "                sn.heatmap(s[:,:,0],cmap = 'Reds', annot=True,cbar = False).invert_yaxis() \n",
    "                \n",
    "                \n",
    "                scur_rollout.append(np.array(z.cpu().detach()))\n",
    "                snext_rollout.append(np.array(next_z.cpu().detach()))\n",
    "                r_rollout.append([0.0])\n",
    "                act_rollout.append([[np.float(m)]])\n",
    "                d_rollout.append([0.0])\n",
    "                \n",
    "\n",
    "            \n",
    "                self.env.render()\n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "                \n",
    "                Curiosityreward = gmm_loss(next_z.detach(), hmus, hsigmas, hlogpi)/33\n",
    "                #floss = criterion(h[m],next_z.detach())\n",
    "                total_loss += Curiosityreward\n",
    "                \n",
    "                \n",
    "                if self.hiddengoals:\n",
    "                    goal_loss.append(criterion(next_z.detach(),z_goal).item()) #how far away the achieved step is from the goal\n",
    "                else:\n",
    "                    goal_loss.append(gmm_loss(z_goal,next_mu,next_logsigma.exp(),torch.tensor([-1.0], dtype = torch.float32).to(self.device))/33)\n",
    "                \n",
    "                \n",
    "                #if next z matches goal to a certain degree then get reward\n",
    "                if goal_loss[-1] < 1.2: \n",
    "                    reward = 4.0\n",
    "                else:\n",
    "                    reward = 0.0\n",
    "                    \n",
    "                \n",
    "                if self.curiosityreward:\n",
    "                    reward = reward + Curiosityreward\n",
    "                \n",
    "                reward = torch.tensor([reward], device=device, requires_grad = False)\n",
    "                total_reward += reward\n",
    "                    \n",
    "                Controllermemory.push(Experience(state.detach(), torch.tensor([m]).to(device), next_state.detach(), reward))\n",
    "                \n",
    "                if Controllermemory.can_provide_sample(batch_size):\n",
    "                    \n",
    "                    # retrieve experiences from batch\n",
    "                    experiences = Controllermemory.sample(batch_size)\n",
    "                    \n",
    "                    states, actions, rewards, next_states = dqn_utils.extract_tensors(experiences)\n",
    "                \n",
    "                    # Get Q Values from policy and target network\n",
    "                    Controllercurrent_q_values = dqn_utils.ActionQValues.get_current(Controllerpolicy_net, states, actions)\n",
    "                    Controllernext_q_values = dqn_utils.ActionQValues.get_next(Controllertarget_net, next_states)\n",
    "                    Controllertarget_q_values = (Controllernext_q_values * gamma) + rewards\n",
    "                    # Calculate Loss and back propagate\n",
    "                    Controllerloss = F.mse_loss(Controllercurrent_q_values, Controllertarget_q_values.unsqueeze(1))\n",
    "                    Controlleroptimizer.zero_grad()\n",
    "                    Controllerloss.backward()\n",
    "                    Controlleroptimizer.step()\n",
    "                    \n",
    "                \n",
    "                if c % target_update == 0:\n",
    "                    print('Updating target network')\n",
    "                    Controllertarget_net.load_state_dict(Controllerpolicy_net.state_dict())\n",
    "            \n",
    "                    \n",
    "            expl_rate = min_exploration_rate + \\\n",
    "            (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*c)\n",
    "            \n",
    "            mdrnnlosses = self.get_loss(torch.tensor(scur_rollout).to(self.device), torch.tensor(act_rollout).to(self.device), torch.tensor(r_rollout).to(self.device),\n",
    "                              torch.tensor(d_rollout).to(self.device), torch.tensor(snext_rollout).to(self.device), include_reward = False)\n",
    "            \n",
    "            \n",
    "           \n",
    "            MDRNNoptimizer.zero_grad()\n",
    "            mdrnnlosses['loss'].backward()\n",
    "                \n",
    "            MDRNNoptimizer.step()\n",
    "            \n",
    "            WM_loss.append(mdrnnlosses['loss'])\n",
    "            \n",
    "            \n",
    "            if goalattempts % 10 == 0:\n",
    "                self.mdrnn.load_state_dict(self.mdrnnBIG.state_dict())    \n",
    "                \n",
    "    \n",
    "            loss_list.append(total_loss/(goalattempts+1))\n",
    "            rollout_reward.append(total_reward)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            plot1 = plt.figure('Average Forward model loss')\n",
    "            plt.plot(loss_list)\n",
    "            plt7= plt.figure('WM_loss')\n",
    "            plt.plot(WM_loss)\n",
    "            plt4 = plt.figure('Distance to goal per step')\n",
    "            plt.cla()\n",
    "            plt.plot(goal_loss)\n",
    "            rolloutrewardplot = plt.figure('Reward per rollout')\n",
    "            plt.plot(self.rollout_reward)\n",
    "            #plt8 = plt.figure('Visitation')\n",
    "            #plt.cla()\n",
    "            #sn.heatmap(visitationarray,cmap = 'Reds', annot=True,cbar = False) \n",
    "            \n",
    "            \n",
    "            plt.show() \n",
    "            \n",
    "            #input('stop')\n",
    "            \n",
    "            \n",
    "            \n",
    "    def transform_obs_hidden(self,obs, hidden,m):\n",
    "        obs = torch.tensor(obs.flatten(),dtype = torch.float32).unsqueeze(0).to(self.device)\n",
    "        #print('obs shape from transform obs hidden: ', obs.shape)\n",
    "       \n",
    "        action =  torch.Tensor([[m]]).to(self.device)\n",
    "        reconx, latent_mu, logsigma = self.vae(obs)\n",
    "        \n",
    "        \n",
    "\n",
    "        sigma = logsigma.exp()\n",
    "        eps = torch.randn_like(sigma)\n",
    "        z = eps.mul(sigma).add_(latent_mu)\n",
    "        \n",
    "        hmus, hsigmas, hlogpi, _, _, next_hidden = self.mdrnn(action, z, tuple(hidden))\n",
    "        \n",
    "        \n",
    "        hlogpi = hlogpi.squeeze()\n",
    "        mixt = Categorical(torch.exp(hlogpi)).sample().item()\n",
    "        \n",
    "        \n",
    "        zh = hmus[:, mixt, :]  + hsigmas[:, mixt, :] * torch.randn_like(hmus[:, mixt, :])\n",
    "        \n",
    "        \n",
    "        \n",
    "        return action.squeeze().cpu().numpy(), next_hidden,z,zh\n",
    "    \n",
    "    def gpcf(self,z_goal,h):\n",
    "        \n",
    "        output = []\n",
    "        for action in range(len(h)):\n",
    "            output.append(criterion(h[action],z_goal).item())\n",
    "            \n",
    "        return output.index(min(output))\n",
    "    \n",
    "   \n",
    "    def get_loss(self, latent_obs, action, reward, terminal,\n",
    "                 latent_next_obs, include_reward: bool):\n",
    "        \"\"\" Compute losses.\n",
    "\n",
    "        The loss that is computed is:\n",
    "        (GMMLoss(latent_next_obs, GMMPredicted) + MSE(reward, predicted_reward) +\n",
    "             BCE(terminal, logit_terminal)) / (LSIZE + 2)\n",
    "        The LSIZE + 2 factor is here to counteract the fact that the GMMLoss scales\n",
    "        approximately linearily with LSIZE. All losses are averaged both on the\n",
    "        batch and the sequence dimensions (the two first dimensions).\n",
    "\n",
    "        :args latent_obs: (BSIZE, SEQ_LEN, LSIZE) torch tensor\n",
    "        :args action: (BSIZE, SEQ_LEN, ASIZE) torch tensor\n",
    "        :args reward: (BSIZE, SEQ_LEN) torch tensor\n",
    "        :args latent_next_obs: (BSIZE, SEQ_LEN, LSIZE) torch tensor\n",
    "\n",
    "        :returns: dictionary of losses, containing the gmm, the mse, the bce and\n",
    "            the averaged loss.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        mus, sigmas, logpi, rs, ds = self.mdrnnBIG(action, latent_obs)\n",
    "        gmm = gmm_loss(latent_next_obs, mus, sigmas, logpi)\n",
    "        bce = F.binary_cross_entropy_with_logits(ds, terminal)\n",
    "        if include_reward:\n",
    "            mse = F.mse_loss(rs, reward)\n",
    "            scale = LSIZE + 2\n",
    "        else:\n",
    "            mse = 0\n",
    "            scale = LSIZE + 1\n",
    "        loss = (gmm + bce + mse) / scale\n",
    "        return dict(gmm=gmm, bce=bce, mse=mse, loss=loss)     \n",
    "        \n",
    "    def tolatent(self,obs):\n",
    "        obs = torch.tensor(obs.flatten(),dtype = torch.float32).unsqueeze(0).to(self.device)\n",
    "        #print('obs shape from transform obs hidden: ', obs.shape)\n",
    "      \n",
    "        reconx, latent_mu, logsigma = self.vae(obs)\n",
    "       \n",
    "        sigma = logsigma.exp()\n",
    "        eps = torch.randn_like(sigma)\n",
    "        z = eps.mul(sigma).add_(latent_mu)\n",
    "       \n",
    "        return latent_mu,logsigma,z\n",
    "    \n",
    "    def tohiddenlatent(self,hidden):\n",
    "        _,latent_mu,logsigma  =  self.HiddenVAE(hidden[0].detach())\n",
    "              \n",
    "        sigma = logsigma.exp()\n",
    "        eps = torch.randn_like(sigma)\n",
    "        zhidden = eps.mul(sigma).add_(latent_mu)\n",
    "        \n",
    "        return zhidden\n",
    "    \n",
    "    def predict_next(self,obs, hidden,m):\n",
    "        obs = torch.tensor(obs.flatten(),dtype = torch.float32).unsqueeze(0).to(self.device)\n",
    "        #print('obs shape from transform obs hidden: ', obs.shape)\n",
    "       \n",
    "        action =  torch.Tensor([[m]]).to(self.device)\n",
    "        reconx, latent_mu, logsigma = self.vae(obs)\n",
    "        \n",
    "        sigma = logsigma.exp()\n",
    "        eps = torch.randn_like(sigma)\n",
    "        z = eps.mul(sigma).add_(latent_mu)\n",
    "        \n",
    "        hmus, hsigmas, hlogpi, _, _, next_hidden = self.mdrnn(action, z, tuple(hidden))\n",
    "        \n",
    "        hlogpi = hlogpi.squeeze()\n",
    "        mixt = Categorical(torch.exp(hlogpi)).sample().item()\n",
    "        zh = hmus[:, mixt, :]  + hsigmas[:, mixt, :] * torch.randn_like(hmus[:, mixt, :])\n",
    "        \n",
    "        return hmus,hsigmas,hlogpi, zh "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
